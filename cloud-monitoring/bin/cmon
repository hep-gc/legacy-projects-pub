#!/usr/bin/env python

import argparse
import json
import logging
import pickle
import pika
import socket
import struct
import sys
import yaml

from datetime import datetime, timedelta
from pymongo import MongoClient
from time import strftime

from cmon import CARBON_PATHS


DEFAULT_CONFIG_FILE = '/etc/cmon/cmon.yml'


def message_callback(ch, method, props, body):
    try:
        msg = json.loads(body)
    except ValueError:
        print 'Invalid message - JSON parsing failed'
        logger.error('Invalid message - JSON parsing failed')

    if 'resources' in msg:
        parse_resources(msg, props.timestamp)
    elif 'clouds' in msg:
        parse_status(msg, props.timestamp)
    else:
        print 'Invalid message - missing a "resources" or "clouds" key'
        logger.error('Invalid message - missing a "resources" or "clouds" key')
        
    # Acknowledge message to prevent re-queueing
    ch.basic_ack(delivery_tag=method.delivery_tag)


def parse_resources(msg, timestamp):
    grid_name = msg['grid']

    # Fetch Current VMs ########################################################

    db_vms = {}
    cursor = db.vms.find({'grid': grid_name, 'last_updated': {'$gte': datetime.now() - timedelta(hours=1)}})
    for vm in cursor:
        db_vms[vm['_id']] = vm

    print msg['resources']


def parse_status(msg, timestamp):
    grid_name = msg['grid']
    print msg['sysinfo']
    grid = {
        'sysinfo': msg['sysinfo'],
        'clouds': {},
        'jobs': {
            'all': {
                'total': 0,
                'unexpanded': 0,
                'idle': 0,
                'lost': 0,
                'unreg': 0,
                'running': 0,
                'removed': 0,
                'completed': 0,
                'held': 0,
                'error': 0,
            }
        },
    }

    # Fetch Current VMs and Jobs ###############################################

    db_vms = {}
    cursor = db.vms.find({'grid': grid_name, 'last_updated': {'$gte': datetime.now() - timedelta(hours=1)}})
    for vm in cursor:
        db_vms[vm['_id']] = vm

    db_jobs = {}
    cursor = db.jobs.find({'grid': grid_name, 'last_updated': {'$gte': datetime.now() - timedelta(hours=1)}})
    for job in cursor:
        db_jobs[job['_id']] = job

    active_vms = []
    active_jobs = []

    # Process Clouds ###########################################################

    logger.debug("clouds: %s", len(msg['clouds']))
    logger.debug("Cloud loop...")

    for cloud in msg['clouds']:
        cloud_name = cloud['name']

        logger.debug("cloud_name: %s", cloud_name)

        grid['clouds'][cloud_name] = {
            'enabled': 1 if cloud['enabled'] else 0,
            'quota':   cloud['max_slots'],
            'vms':     {},
            'slots':   {},
            'jobs': {
                'all': {
                    'total': 0,
                    'unexpanded': 0,
                    'idle': 0,
                    'lost': 0,
                    'unreg': 0,
                    'running': 0,
                    'removed': 0,
                    'completed': 0,
                    'held': 0,
                    'error': 0,
                }
            },
            'vmtypes': {},
            'mips':    0,
            'kflops':  0,
        }
        
        logger.debug("cloud_vms: %s",len(cloud['vms']))
        logger.debug("VMs loop...")

        for vm in cloud['vms']:
            vm_id = vm['hostname']

            # Ignore VMs without a hostname
            if not vm_id:
                logger.debug("No vms to process")
                continue
            
            active_vms.append(vm_id)
            # Find or create a document for this VM
            db_vm = db_vms.get(vm_id)
            if db_vm:
                db_vm_status = db_vm['status']
                vm_doc = db_vm
            else:
                vm_doc = {
                    '_id': vm_id,
                    'first_updated': datetime.now(),
                    'grid': grid_name,
                    'cloud': cloud_name,
                    'hostname': vm['hostname'],
                    'id': vm['id'],
                    'type': vm['vmtype'],
                    'alt_hostname': vm['alt_hostname'],
                    'initialize_time': datetime.fromtimestamp(vm['initialize_time']),
                }
            vm_doc['last_updated'] = datetime.now()

            vm_status = vm['status'].lower()
            if vm['override_status']:
                vm_status = vm['override_status'].lower()

            vm_doc['status'] = vm_status
            if db_vm:
                vm_status_history = db_vm.get('status_history', [])
                if vm_status != db_vm_status:
                    vm_status_history.append([datetime.now(), vm_status])
            else:
                vm_status_history = [[datetime.now(), vm_status]]

            vm_doc['status_history'] = vm_status_history

            db.vms.replace_one({'_id': vm_id}, vm_doc, True)

            if vm['vmtype'] not in grid['clouds'][cloud_name]['vmtypes']:
                grid['clouds'][cloud_name]['vmtypes'][vm['vmtype']] = {
                    'total':    0,
                    'total2':   0,
                    'starting': 0,
                    'running':  0,
                    'retiring': 0,
                    'error':    0,
                    'shutdown': 0,
                    'unknown': 0,
                }

            grid['clouds'][cloud_name]['vms'][vm['hostname'].split('.')[0]] = {
                'status': vm_status,
                'vmtype': vm['vmtype'],
                'count':  0,
            }

            grid['clouds'][cloud_name]['vmtypes'][vm['vmtype']]['total'] += 1
            if vm_status in grid['clouds'][cloud_name]['vmtypes'][vm['vmtype']].keys():
                grid['clouds'][cloud_name]['vmtypes'][vm['vmtype']][vm_status] += 1
            else:
                grid['clouds'][cloud_name]['vmtypes'][vm['vmtype']]['unknown'] += 1

        cloud_id = '.'.join([str(grid_name), str(cloud_name)])
        logger.debug("cloud_id: %s", cloud_id)
        cloud_doc = {
            '_id': cloud_id,
            'grid': grid_name,
            'cloud': cloud_name,
            'type': cloud['cloud_type'],
            'enabled': cloud['enabled'],
            'quota': cloud['max_slots'],
        }
        
        db.clouds.replace_one({'_id': cloud_id}, cloud_doc, True)

    # Process Jobs #############################################################

    logger.debug("jobs: %s", len(msg['jobs']))
    logger.debug("Jobs loop...")

    for job in msg['jobs']:
        job_id = job['id']
        job_host = job.get('remote_host')
        job_last_host = job.get('last_remote_host')

        active_jobs.append(job_id)

        db_job = db_jobs.get(job_id)
        if db_job:
            db_job_status = db_job['status']
            db_job_host = db_job.get('host')
            job_doc = db_job
        else:
            job_doc = {
                '_id': job_id,
                'first_updated': datetime.now(),
                'grid': grid_name,
                'queue_date': datetime.fromtimestamp(job['queue_date'])
            }

        job_doc['last_updated'] = datetime.now()

        job_doc['status'] = job['status']
        job_doc['last_host'] = job_last_host
        job_doc['host'] = job_host

        if job_host or job_last_host:
            if job_host and job_host in db_vms:
                job_doc['cloud'] = db_vms[job_host]['cloud']
            elif job_last_host and job_last_host in db_vms:
                job_doc['cloud'] = db_vms[job_last_host]['cloud']
        else:
            if not db_job or not db_job.get('cloud'):
                job_doc['cloud'] = None

        if db_job:
            job_status_history = db_job.get('status_history', [])
            job_host_history = db_job.get('host_history', [])

            if job['status'] != db_job_status:
                logger.debug('%s - %s %s -> %s', grid_name, job_id, db_job_status, job['status'])
                job_status_history.append([datetime.now(), job['status']])

            if job_host != db_job_host:
                logger.debug('%s - %s %s -> %s', grid_name, job_id, db_job_host, job_host)
                job_host_history.append([datetime.now(), job_host])
        else:
            logger.debug('%s - %s %s %s', grid_name, job_id, job['status'], job_host)
            job_status_history = [[datetime.now(), job['status']]]
            job_host_history = [[datetime.now(), job_host]]

        job_doc['status_history'] = job_status_history
        job_doc['host_history'] = job_host_history

        job_type = None

        if 'target_clouds' in job:
            if job['target_clouds'] == 'IAAS':
                job_type = '1_Core'
            elif job['target_clouds'] == 'IAAS_MCORE':
                job_type = '8_Core'
            elif job['target_clouds'] == 'Alberta':
                job_type = 'Alberta'
            elif job['target_clouds'] == 'CERNClouds':
                if job['accounting_group'] == 'group_mcore':
                    job_type = 'MCore'
                elif job['accounting_group'] == 'group_himem':
                    job_type = 'Himem'
                elif job['accounting_group'] == 'group_analysis':
                    job_type = 'Analy'
            elif job['target_clouds'] == 'cern-preservation':
                job_type = 'DPHEP'

        job_doc['type'] = job_type

        db.jobs.replace_one({'_id': job_id}, job_doc, True)

        grid['jobs']['all']['total'] += 1
        grid['jobs']['all'][job['status']] += 1

        if job_type:
            if job_type not in grid['jobs']:
                grid['jobs'][job_type] = {
                    'unexpanded': 0,
                    'idle': 0,
                    'lost': 0,
                    'unreg': 0,
                    'running': 0,
                    'removed': 0,
                    'completed': 0,
                    'held': 0,
                    'error': 0,
                }
                grid['jobs'][job_type]['total'] = 0

            grid['jobs'][job_type]['total'] += 1
            grid['jobs'][job_type][job['status']] += 1

        if 'cloud' in job_doc and job_doc['cloud']:
            grid['clouds'][job_doc['cloud']]['jobs']['all']['total'] += 1
            grid['clouds'][job_doc['cloud']]['jobs']['all'][job['status']] += 1

    # Process Slots ############################################################

    logger.debug("slot: %s", len(msg['slots']))
    logger.debug("Slots loop...")

    for slot in msg['slots']:

        if '@' not in slot['name']:
            continue

        (slot_name, machine) = slot['name'].split('@')

        machine = machine.split('.')[0]

        for cloud_name, cloud in grid['clouds'].iteritems():
            if machine in cloud['vms']:

                vmtype = cloud['vms'][machine]['vmtype']

                if '_' not in slot_name:
                    grid['clouds'][cloud_name]['vmtypes'][vmtype]['total2'] +=1
                    continue

                else:
                    cloud['vms'][machine]['count'] += 1

                    if vmtype not in cloud['slots']:
                        cloud['slots'][vmtype] = {}

                    if slot_name in cloud['slots'][vmtype]:
                        cloud['slots'][vmtype][slot_name] += 1
                    else:
                        cloud['slots'][vmtype][slot_name] = 1


    # Process Summary ##########################################################

    #logger.debug("grid[clouds]: %s", grid['clouds'])       
    logger.debug("Process Summary loop...")

    for cloud_name, cloud in grid['clouds'].iteritems():
        idle = dict((vmtype, 0) for vmtype in cloud['vmtypes'].keys())
        idle['total'] = 0

        lost = dict((vmtype, 0) for vmtype in cloud['vmtypes'].keys())
        lost['total'] = 0

        unreg = dict((vmtype, 0) for vmtype in cloud['vmtypes'].keys())
        unreg['total'] = 0

        if len(cloud['vms']):
            for vm_id, vm in cloud['vms'].iteritems():
                if vm['status'] == 'running' and vm['count'] == 0:
                    idle['total'] += 1
                    idle[vm['vmtype']] += 1

            missing = grid['clouds'][cloud_name]['vmtypes'][vm['vmtype']]['total2'] - grid['clouds'][cloud_name]['vmtypes'][vm['vmtype']]['total']

            if missing >= 0:

                lost['total'] +=  missing
                lost[vm['vmtype']] = missing
            else:
                unreg['total'] +=  abs(missing)
                unreg[vm['vmtype']] = abs(missing)



        cloud.pop('vms', None)
        cloud['idle'] = idle
        cloud['lost'] = lost
        cloud['unreg'] = unreg

    grid['last_updated'] = datetime.now()

    logger.debug("grid_name: %s", grid_name)
    db.grids.replace_one({'_id': grid_name}, grid, True)

    # Store Raw Data ###########################################################

    db.status.replace_one({'_id': grid_name}, msg, True)

    # Mark Inactive VMs ########################################################
    logger.debug("Inactive VMs loop...")

    for vm_id, db_vm in db_vms.iteritems():
        if vm_id not in active_vms and db_vm['status'] != 'gone':
            vm_doc = db_vm
            vm_doc['status'] = 'gone'
            vm_doc['status_history'].append([datetime.now(), 'gone'])

            logger.debug('%s - %s/%s %s', grid_name, db_vm['cloud'], vm_id, 'gone')
            db.vms.replace_one({'_id': vm_id}, vm_doc, True)

    # Mark Inactive Jobs #######################################################
    logger.debug("Inactive jobs loop...")

    for job_id, db_job in db_jobs.iteritems():
        if job_id not in active_jobs and db_job['status'] != 'gone':
            job_doc = db_job
            job_doc['status'] = 'gone'
            job_doc['status_history'].append([datetime.now(), 'gone'])

            logger.debug('%s - %s %s', grid_name, job_id, 'gone')
            db.jobs.replace_one({'_id': job_id}, job_doc, True)

    # Update Graphite ##########################################################

    metrics = {}
    logger.debug("Update Graphite loop...")
    for cloud_name, cloud in grid['clouds'].iteritems():
        for metric in ['enabled', 'quota', 'mips', 'kflops']:
            path = CARBON_PATHS['cloud_other'].format(grid_name, cloud_name, metric)
            metrics[path] = cloud[metric]

        for vmtype, slots in cloud['slots'].iteritems():
            for slot, count in slots.iteritems():
                path = CARBON_PATHS['cloud_slots'].format(grid_name, cloud_name, vmtype, slot)
                metrics[path] = count

        for vmtype, count in cloud['idle'].iteritems():
            path = CARBON_PATHS['cloud_idle'].format(grid_name, cloud_name, vmtype)
            metrics[path] = count
        
        for vmtype, count in cloud['lost'].iteritems():
            path = CARBON_PATHS['cloud_lost'].format(grid_name, cloud_name, vmtype)
            metrics[path] = count

        for vmtype, count in cloud['unreg'].iteritems():
            path = CARBON_PATHS['cloud_unreg'].format(grid_name, cloud_name, vmtype)
            metrics[path] = count

        for vmtype, statuses in cloud['vmtypes'].iteritems():
            for status, count in statuses.iteritems():
                path = CARBON_PATHS['cloud_vms'].format(grid_name, cloud_name, vmtype, status)
                metrics[path] = count

        for jobtype, statuses in cloud['jobs'].iteritems():
            for status, count in statuses.iteritems():
                path = CARBON_PATHS['cloud_jobs'].format(grid_name, cloud_name, jobtype, status)
                metrics[path] = count

    for jobtype, statuses in grid['jobs'].iteritems():
        for status, count in statuses.iteritems():
            path = CARBON_PATHS['jobs'].format(grid_name, jobtype, status)
            metrics[path] = count

    for stattype, status in grid['sysinfo'].iteritems():
        path = CARBON_PATHS['sysinfo'].format(grid_name, stattype)
        metrics[path] = status

    graphite_metrics = []
    for path, count in metrics.iteritems():
        graphite_metrics.append((path, (timestamp, count)))

    try:
        sock = socket.socket()
        sock.connect((config['graphite']['server'], config['graphite']['pickle_port']))
    except socket.error:
        raise SystemExit("Couldn't connect to {} on port {}, is carbon-cache.py running?".format(config['graphite']['server'], config['graphite']['pickle_port']))

    package = pickle.dumps(graphite_metrics, 1)
    length  = struct.pack('!L', len(package))

    logger.info('{} - {} metrics sent to Graphite'.format(grid_name, len(graphite_metrics)))

    sock.sendall(length + package)
    sock.close()


# Configure C'mon ##############################################################

parser = argparse.ArgumentParser(description="C'mon: monitor your clouds")
parser.add_argument('--debug', action='store_true')
parser.add_argument('--config-file', type=str, default=DEFAULT_CONFIG_FILE)
args = parser.parse_args()

with open(args.config_file, 'r') as config_file:
    config = yaml.load(config_file)

logger = logging.getLogger('cmon')
logger.setLevel(logging.DEBUG)

formatter = logging.Formatter("%(asctime)s %(name)s [%(levelname)8s] %(message)s")

handler_stream = logging.StreamHandler()
handler_stream.setFormatter(formatter)
logger.addHandler(handler_stream)

handler_file = logging.FileHandler(config['log_file'])
handler_file.setFormatter(formatter)
logger.addHandler(handler_file)


# Connect to DB and RabbitMQ ###################################################

logger.info("Starting C'mon")

logger.info('Connecting to MongoDB %s:%s/%s', config['mongodb']['server'], config['mongodb']['port'], config['mongodb']['db'])
db = MongoClient(config['mongodb']['server'], config['mongodb']['port'])[config['mongodb']['db']]

logger.info('Connecting to RabbitMQ %s@%s:%s%s', config['rabbitmq']['user'], config['rabbitmq']['server'], config['rabbitmq']['port'], config['rabbitmq']['vhost'])
creds = pika.PlainCredentials(config['rabbitmq']['user'], config['rabbitmq']['secret'])
params = pika.ConnectionParameters(config['rabbitmq']['server'], config['rabbitmq']['port'], config['rabbitmq']['vhost'], creds, backpressure_detection=config['rabbitmq']['backpressure'])
#params = pika.ConnectionParameters(config['rabbitmq']['server'], config['rabbitmq']['port'], config['rabbitmq']['vhost'], creds)

rmq = pika.BlockingConnection(params)

exchange = config['rabbitmq']['exchange']
queue = config['rabbitmq']['queue']

channel = rmq.channel()
channel.exchange_declare(exchange=exchange, exchange_type='fanout')
channel.queue_declare(queue=queue, durable=True)
channel.queue_bind(exchange=exchange, queue=queue)


# Start Consuming Messages #####################################################

try:
    logger.info('Consuming message queue...')
    channel.basic_consume(message_callback, queue=queue)
    logger.debug('Queue: %s', queue)
    channel.start_consuming()
    logger.info('Done eating the queue.')
except KeyboardInterrupt:
    logger.info('Shutting down...')

finally:
    rmq.close()
